# Spezialisierte Tabellen-Pipeline Integration

## üéØ √úbersicht

Du hast jetzt eine **komplette spezialisierte Pipeline** f√ºr Excel/Tabellen mit:

### üì¶ Komponenten

1. **TableExtractionService** - Spezialisiert f√ºr Excel/Tabellen-Extraktion
2. **TableChunkingService** - Spezialisiert f√ºr Tabellen-Chunking (beh√§lt QA-Paare zusammen)
3. **TableFAQIngestionService** - Vollst√§ndige Pipeline f√ºr Tabellen
4. **Enhanced get_faq_ingestion_service** - Integration in bestehende Struktur

### üîß Services-Verwendung

- **TableExtractionService** (statt UTF8ExtractionService)
- **TableChunkingService** (statt normaler ChunkingService)
- **AzureOpenAIEmbeddingService** (bestehender)
- **PostgreSQLVectorService** (bestehender)
- **Kein SummaryService** (nicht ben√∂tigt f√ºr Tabellen)

## üöÄ Integration in deine bestehende Struktur

### 1. Dateien kopieren

```bash
# Kopiere die spezialisierten Services in dein Projekt
cp integration/table_extraction_service.py faq/services/
cp integration/table_chunking_service.py faq/services/
cp integration/table_faq_ingestion_service.py faq/services/
cp integration/enhanced_faq_ingestion_service.py faq/services/
```

### 2. get_faq_ingestion_service erweitern

```python
# In deiner bestehenden get_faq_ingestion_service Funktion
def get_faq_ingestion_service(service_type: str = "default", **kwargs):
    if service_type == "table":
        # SPEZIALISIERTE TABELLEN-PIPELINE
        embedding_service = get_embedding_service()  # AzureOpenAIEmbeddingService
        vector_service = get_vector_service()        # PostgreSQLVectorService
        
        return create_table_faq_ingestion_service(
            embedding_service=embedding_service,
            vector_service=vector_service,
            max_chunk_size=kwargs.get('max_chunk_size', 1000),
            overlap=kwargs.get('overlap', 100)
        )
    
    elif service_type == "default":
        # DEIN BESTEHENDER SERVICE
        return get_default_faq_ingestion_service(**kwargs)
    
    else:
        raise ValueError(f"Unbekannter Service-Type: {service_type}")
```

### 3. API-Endpoint erweitern

```python
# In deinem upload_document Endpoint
@router.post("/upload")
async def upload_document(
    file: UploadFile,
    document_source: str,
    service_type: str = "default"  # NEU: "table" f√ºr Tabellen
):
    # Service basierend auf Dateityp und service_type w√§hlen
    if file.filename.lower().endswith(('.xlsx', '.xls', '.xlsm', '.xlsb', '.csv')):
        service_type = "table"  # Automatisch Tabellen-Pipeline f√ºr Excel
    
    # Service erstellen
    service = get_faq_ingestion_service(
        service_type=service_type,
        embedding_service=get_embedding_service(),
        vector_service=get_vector_service()
    )
    
    # Dokument verarbeiten
    result = await service.async_ingest(
        filename=file.filename,
        raw_content=await file.read(),
        documentId=generate_document_id(),
        documentSource=document_source
    )
    
    return result
```

## üéØ Vorteile der spezialisierten Pipeline

### ‚úÖ F√ºr Tabellen optimiert
- **Multi-Sheet-Unterst√ºtzung**: Erkennt automatisch Test_Chatbot, FAQ_DEUTSCH, FAQ_English
- **QA-Paar-Erhaltung**: Beh√§lt Frage-Antwort-Paare als Einheiten zusammen
- **Strukturierte Chunks**: Jeder Chunk enth√§lt vollst√§ndige QA-Paare
- **Fehlerbehandlung**: Detaillierte Validierung und Fehlermeldungen

### ‚úÖ Bestehende Services wiederverwendet
- **AzureOpenAIEmbeddingService**: F√ºr Embedding-Generierung
- **PostgreSQLVectorService**: F√ºr Vector-Storage
- **Keine Duplikation**: Nutzt deine bestehende Infrastruktur

### ‚úÖ Nahtlose Integration
- **Proxy-Pattern**: Entscheidet automatisch zwischen Tabellen und Standard-Pipeline
- **Backward Compatibility**: Bestehende Funktionalit√§t bleibt unver√§ndert
- **Flexible Konfiguration**: Chunk-Gr√∂√üe, Overlap, etc. konfigurierbar

## üìã Verwendung

### Automatische Erkennung
```python
# Excel-Datei wird automatisch als "table" erkannt
service = get_faq_ingestion_service(service_type="table")
```

### Manuelle Auswahl
```python
# Explizit Tabellen-Pipeline verwenden
service = get_faq_ingestion_service(
    service_type="table",
    max_chunk_size=1500,
    overlap=200
)
```

### Standard-Pipeline
```python
# Bestehende Pipeline f√ºr andere Dateitypen
service = get_faq_ingestion_service(service_type="default")
```

## üîç Multi-Sheet-Unterst√ºtzung

Die Pipeline erkennt automatisch:

1. **Test_Chatbot** (Priorit√§t 1) - Dein Blatt mit Nr., Frage, Antwort
2. **FAQ_DEUTSCH** (Priorit√§t 2)
3. **FAQ_English** (Priorit√§t 3)

Falls kein erwartetes Blatt gefunden wird, durchsucht sie alle anderen Bl√§tter nach der richtigen Struktur.

## üìä Ergebnis

Nach der Verarbeitung erh√§ltst du:

- **Strukturierte QA-Paare** aus dem erkannten Tabellenblatt
- **Optimierte Chunks** mit vollst√§ndigen QA-Paaren
- **Detaillierte Metadaten** mit Tabellen-spezifischen Informationen
- **Fehlerbehandlung** f√ºr ung√ºltige Strukturen
- **Vector-Storage** mit semantischer Suche

Die spezialisierte Pipeline ist jetzt bereit f√ºr die Integration in dein bestehendes System! üöÄ
